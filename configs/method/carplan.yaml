# common
model_name: carplan
ckpt_path: null
# ckpt_path: 'checkpoints/pluto.ckpt'
# action space
action_space:
    dynamic_type: waypoint
    type: continuous # can be 'continuous' or 'discrete',
    action_ranges: [[-0.14, 6], [-0.35, 0.35], [-0.15,0.15]]
    bins: [13,13,13] #only for discrete
# data related
max_len: 2 # max_len is the how long you want to use the history for your prediction

# model related
mode: 6 # mode for GMM
num_of_decoder: 3 # how many decoder to use in MPP
est_layer: 0 # which layer to estimate the action

max_length: ${max_len} # for inference
eval_context_length: ${max_len}  #eval_context_length is the how long you want to use the history for your prediction
function: 'enc-LWM-MPP' # can be: 'enc-LWM-MPP', 'enc-MPP', indicates which combination you wish to use

pretrain_world: null
freeze_world: False # whether to freeze the world model during training

pretrain_enc: null #'checkpoints/pretrained_bert.pth.tar'
freeze_enc: False # whether to freeze the encoder during training

dim: 128
state_channel: 6
polygon_channel: 6
history_channel: 9
history_steps: 10
future_steps: 80
encoder_depth: 4
decoder_depth: 4
drop_path: 0.2
dropout: 0.1
num_heads: 4
num_modes: 12
state_dropout: 0.75
use_ego_history: false
state_attn_encoder: true
use_hidden_proj: True
ref_free_traj: True
# spa args
use_collision_loss: True
cat_x: False

is_simulation: False
deepseek_num_experts_per_tok: 2
deepseek_n_routed_experts: 16
deepseek_scoring_func: 'softmax'
deepseek_aux_loss_alpha: 0.001
deepseek_seq_aux: False
deepseek_norm_topk_prob: True
deepseek_hidden_size: 128
deepseek_intermediate_size: 128
deepseek_moe_intermediate_size: 80
deepseek_n_shared_experts: 2
deepseek_first_k_dense_replaces: 1
deepseek_road_balance_loss: True
deepseek_road_balance_loss_weight: 2

router_original_init: True

deepseek_residual: False
use_av_query_for_displacement: True

collision_loss_weight: 1.0
cil_loss_weight: 1.0
planning_loss_weight: 1.0
prediction_loss_weight: 2.0

is_carplan: False

defaults:
    - encoder: bert
    - world: latent_world_model

# train
max_epochs: 50 #150 #40
learning_rate: 1.0e-4
# train_batch_size: 5000 #150
train_batch_size: 32 #150
bert_chunk_size: 128
# bert_chunk_size: 512

grad_clip_norm: null
optimizer:
    type: Adam
    # eps: 0.0001
scheduler:
    type: CosineAnnealingLR

strategy: ddp_find_unused_parameters_true
# strategy: ddp
